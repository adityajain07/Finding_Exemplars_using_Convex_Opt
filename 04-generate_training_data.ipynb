{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Authors      : Aditya Jain and Safwan Jamal\n",
    "Date started : November 15, 2022\n",
    "About        : Convex Optimization project; generate training data lists for different lambda\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save list for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_list(data_dir, save_dir, no_imgs_per_class=40):\n",
    "    \"\"\"\n",
    "    saves test images list \n",
    "    \n",
    "    Args:\n",
    "        data_dir         : root directory containing the data\n",
    "        no_imgs_per_class: number of images to save; Optional; default=150 \n",
    "        save_dir         : directory to save the list\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []    \n",
    "    for category in os.listdir(data_dir): \n",
    "        image_names = os.listdir(data_dir + category)[:no_imgs_per_class]        \n",
    "        for image in image_names:\n",
    "            data.append([image, category])\n",
    "    \n",
    "    data_df = pd.DataFrame(data, columns =['image', 'category'])\n",
    "    data_df.to_csv(save_dir + 'test.csv', index=False)\n",
    "    \n",
    "cifar_data_dir = './cifar-10-images/test/'\n",
    "save_list_dir  = './image_list_cifar/'\n",
    "test_list(cifar_data_dir, save_list_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save list for all train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_list_all(data_dir, save_dir, no_imgs_per_class=150):\n",
    "    \"\"\"\n",
    "    saves training images list for all of the training set\n",
    "    \n",
    "    Args:\n",
    "        data_dir         : root directory containing the data        \n",
    "        save_dir         : directory to save the list\n",
    "        no_imgs_per_class: number of images to save; Optional; default=150 \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []    \n",
    "    for category in os.listdir(data_dir): \n",
    "        image_names = sorted(os.listdir(data_dir + category))[:no_imgs_per_class]        \n",
    "        for image in image_names:\n",
    "            data.append([image, category])\n",
    "    \n",
    "    data_df = pd.DataFrame(data, columns =['image', 'category'])\n",
    "    data_df.to_csv(save_dir + 'train_full.csv', index=False)\n",
    "    \n",
    "cifar_data_dir = './cifar-10-images/train/'\n",
    "save_list_dir = './image_list_cifar/'\n",
    "train_list_all(cifar_data_dir, save_list_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save list for random train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_list_random(data_dir, save_dir, percent, no_imgs_per_class=150):\n",
    "    \"\"\"\n",
    "    saves training images list for a random subset of the training set\n",
    "    \n",
    "    Args:\n",
    "        data_dir : root directory containing the data \n",
    "        save_dir : directory to save the list\n",
    "        percent  : percentage of samples to sample from training set\n",
    "        no_imgs_per_class: number of images in full training set; Optional; default=150 \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []    \n",
    "    for category in os.listdir(data_dir): \n",
    "        image_names  = sorted(os.listdir(data_dir + category))[:no_imgs_per_class] \n",
    "        random.shuffle(image_names)\n",
    "        total_images = len(image_names)\n",
    "        image_names  = image_names[:int(percent*total_images)]\n",
    "        for image in image_names:\n",
    "            data.append([image, category])\n",
    "    \n",
    "    data_df = pd.DataFrame(data, columns =['image', 'category'])\n",
    "    data_df.to_csv(save_dir + 'train_random_per_' + str(int(percent*100)) + '.csv', index=False)\n",
    "    \n",
    "cifar_data_dir = './cifar-10-images/train/'\n",
    "save_list_dir = './image_list_cifar/'\n",
    "percent        = 0.8\n",
    "train_list_random(cifar_data_dir, save_list_dir, percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save list for exemplar train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero rows are 10\n"
     ]
    }
   ],
   "source": [
    "def train_list_cvopt_exemplar(image_data_dir, diss_data_dir, category, save_dir, percent, lambda_fac):\n",
    "    \"\"\"\n",
    "    saves training images list for a particular class given the percetage value needed\n",
    "    \n",
    "    Args:\n",
    "        image_data_dir   : root directory containing the image data\n",
    "        diss_data_dir    : directory containing the dissimilarity data\n",
    "        category         : class for which optimization needs to be done\n",
    "        save_dir         : directory to save the list\n",
    "        percent          : percentage of training points needed\n",
    "        lambda_fac       : lambda factor to be applied to lambda_max\n",
    "    \"\"\"\n",
    "    \n",
    "    filename  = open(diss_data_dir + category + '_dissimilarity_matrix_150x150.pickle', 'rb')\n",
    "    D         = pickle.load(filename)\n",
    "    n         = len(D)\n",
    "    \n",
    "    # calculate lambda_max\n",
    "    lambda_max = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            lambda_cur = np.linalg.norm(D[i, :] - D[j, :], ord=1)/2  #l-infinity norm\n",
    "        if lambda_cur>lambda_max:\n",
    "            lambda_max = lambda_cur\n",
    "    \n",
    "    ## convex optimization\n",
    "    # variable definitions\n",
    "    Z             = cp.Variable((n, n))\n",
    "    lambda_t      = lambda_fac*lambda_max\n",
    "\n",
    "    # objective function\n",
    "    cost_encoding = cp.trace(D.T@Z)        # cost of encoding all data points using representatives\n",
    "    cost_no_repr  = 0                      # cost associated with no. of representatives\n",
    "    for i in range(n):\n",
    "        cost_no_repr += cp.max(Z[i, :])  # l-infinity norm\n",
    "    cost_no_repr = lambda_t*cost_no_repr\n",
    "\n",
    "    # objective function\n",
    "    objective    = cp.Minimize(cost_encoding + cost_no_repr)\n",
    "\n",
    "    # constraints\n",
    "    # probab. should be >=0\n",
    "    # probabilities should sum to one for every column\n",
    "    constraints = [Z>=0, np.ones((1,n))@Z == np.ones((1,n))]\n",
    "\n",
    "    # optimization program\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()    \n",
    "    Z = np.round(Z.value, 2)\n",
    "    \n",
    "    # fetching the specific image names from original data\n",
    "    filename  = open(diss_data_dir + category + '_image_list_150x150.pickle', 'rb')\n",
    "    data      = pickle.load(filename)\n",
    "    orig_image_set = sorted(data)\n",
    "    \n",
    "    # check for non-zero rows\n",
    "    num_non_zeros_rows = 0\n",
    "    img_idx_list       = []\n",
    "    data               = [] \n",
    "    for i in range(n):\n",
    "        z_row = Z[i, :]\n",
    "        if np.any(z_row):\n",
    "            num_non_zeros_rows += 1\n",
    "            img_idx_list.append(i)\n",
    "            data.append([orig_image_set[i], category]) \n",
    "            \n",
    "    print(f'Number of non-zero rows are {num_non_zeros_rows}')\n",
    "#     print(img_idx_list)\n",
    "    \n",
    "    # save training list\n",
    "    data_df = pd.DataFrame(data, columns =['image', 'category'])\n",
    "    data_df.to_csv(save_dir + 'train_exemplar_' + category + '_per_' + str(int(percent*100)) + '.csv', index=False)\n",
    "    \n",
    "\n",
    "cifar_data_dir = './cifar-10-images/train/'\n",
    "diss_data_dir  = './dissimilarity_data_cifar/'\n",
    "category       = 'airplane'\n",
    "save_list_dir  = './image_list_cifar/'\n",
    "percent        = 0.2\n",
    "lambda_fac     = 0.05\n",
    "train_list_cvopt_exemplar(cifar_data_dir, diss_data_dir, category, save_list_dir, percent, lambda_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (milamoth_ai)",
   "language": "python",
   "name": "milamoth_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
